
// Pre: Read File Into DataFrame:

// please change the path
val transactionsDF = spark.read.format("csv").option("sep", ",")
.option("header", "false")
.load("/home/leo/IdeaProjects/DS503/DS503-Project3/input/Transactions")
val newColumns = Seq("TransID","CustID","TransTotal","TransNumItems","TransDesc")
val temp1 = transactionsDF.toDF(newColumns:_*)
// change column type of the dataframe
val TransactionsDF = temp1.withColumn("TransID", temp1("TransID").cast(org.apache.spark.sql.types.IntegerType))
.withColumn("CustID", temp1("CustID").cast(org.apache.spark.sql.types.IntegerType))
.withColumn("TransTotal", temp1("TransTotal").cast(org.apache.spark.sql.types.FloatType))
.withColumn("TransNumItems", temp1("TransNumItems").cast(org.apache.spark.sql.types.IntegerType))

// have a glance at the dataframe contents
TransactionsDF.show

// have a glance at the TransactionDataFrame's structure
TransactionsDF.printSchema


// (1) T1:
val result1 = TransactionsDF.filter("TransTotal < 200")
result1.show

// (2) T2:
result1.createOrReplaceTempView("Transactions")
val summary1=spark.sql("Select TransNumItems,sum(TransTotal),avg(TransTotal),min(TransTotal),max(TransTotal) from Transactions group by TransNumItems")

// (3):
summary1.show

// (4) T3:
val summary2=spark.sql("SELECT CustID,COUNT(CustID) as count1 FROM Transactions Group By CustID")


// (5) T4:
val result2 = TransactionsDF.filter("TransTotal < 600")

// (6) T5:
result2.createOrReplaceTempView("Transactions2")
val summary3=spark.sql("SELECT CustID,COUNT(CustID) as count2 FROM Transactions2 Group By CustID")

// (7) T6:
summary2.createOrReplaceTempView("TC1")
summary3.createOrReplaceTempView("TC2")
val summary4=spark.sql("SELECT TC1.CustID FROM TC1,TC2 WHERE TC1.CustID=Tc2.CustID and TC1.count1>(5*TC2.count2)")


// (8):
summary4.show
